{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b257a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y numpy pandas tensorflow\n",
    "%pip install --no-cache-dir tensorflow==2.12.0 numpy==1.23.5 pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "import io\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b114ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'manisAI.pth'\n",
    "model = keras.models.load_model(model_filename)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ae65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filename = 'labels.txt'\n",
    "labels = {}\n",
    "with open(labels_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            idx, label = line.strip().split(' ', 1)\n",
    "            labels[int(idx)] = label\n",
    "\n",
    "print(f\"Loaded {len(labels)} classes:\")\n",
    "for idx, label in labels.items():\n",
    "    print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02748080",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'Testing/'\n",
    "test_images = []\n",
    "for root, _, files in os.walk(test_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            test_images.append(os.path.join(root, file))\n",
    "\n",
    "test_images.sort()  # Sort to ensure consistent order\n",
    "print(f\"Found {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "input_shape = model.input_shape[1:3]\n",
    "\n",
    "for img_path in tqdm(test_images):\n",
    "    try:\n",
    "        # Preprocess the image\n",
    "        img = image.load_img(img_path, target_size=input_shape)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = img_array / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Predict class probabilities\n",
    "        pred_probs = model.predict(img_array, verbose=0)[0]  # shape: (n_classes,)\n",
    "        predicted_class_idx = int(np.argmax(pred_probs))\n",
    "\n",
    "        # Get the label for the predicted class\n",
    "        predicted_label = labels.get(predicted_class_idx, f\"Unknown ({predicted_class_idx})\")\n",
    "\n",
    "        # Store prediction result\n",
    "        predictions.append({\n",
    "            'image': os.path.basename(img_path),\n",
    "            'predicted_class_index': predicted_class_idx,\n",
    "            'predicted_label': predicted_label,\n",
    "            'class_probabilities': pred_probs.tolist()  # convert to list for JSON-safe export\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {str(e)}\")\n",
    "        predictions.append({\n",
    "            'image': os.path.basename(img_path),\n",
    "            'predicted_class_index': -1,\n",
    "            'predicted_label': 'Error',\n",
    "            'class_probabilities': []\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(predictions)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score,\n",
    "    roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# True and predicted labels\n",
    "true_labels = [1, 2, 5]  # Adjust this list to match your full test set\n",
    "results_df['true_class_index'] = true_labels\n",
    "y_true = results_df['true_class_index'].astype(int).values\n",
    "y_pred = results_df['predicted_class_index'].astype(int).values\n",
    "y_probs = np.array(results_df['class_probabilities'].tolist())\n",
    "\n",
    "## Quick fix for ROC Curve as I only have 3 classes here (DO NOT NEED THIS IF YOU HAVE 8 CLASSES IN YOUR TEST SET)\n",
    "FULL_NUM_CLASSES = 8  # total number of possible classes\n",
    "\n",
    "# Pad probability vectors to length 8\n",
    "def pad_probs(probs, target_len=FULL_NUM_CLASSES):\n",
    "    padded = np.zeros(target_len)\n",
    "    padded[:len(probs)] = probs  # assumes probs are in order (class 0, 1, 2, ...)\n",
    "    return padded\n",
    "\n",
    "# Apply padding\n",
    "y_probs_padded = np.array([pad_probs(p, FULL_NUM_CLASSES) for p in results_df['class_probabilities']])\n",
    "\n",
    "# Update your DataFrame or use directly in metrics\n",
    "y_probs = y_probs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "n_classes = FULL_NUM_CLASSES\n",
    "class_names = list(range(FULL_NUM_CLASSES))\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n‚úÖ Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1 per class & macro\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=class_names, average=None)\n",
    "macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nüìä Per-class metrics:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"Class {cls}: Precision={prec[i]:.4f}, Recall={rec[i]:.4f}, F1={f1[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nüì¶ Macro Precision: {macro_prec:.4f}, Macro Recall: {macro_rec:.4f}, Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "# ROC AUC (requires binarized labels)\n",
    "y_true_bin = label_binarize(y_true, classes=class_names)\n",
    "\n",
    "# ROC AUC per class and macro\n",
    "try:\n",
    "    auc_per_class = roc_auc_score(y_true_bin, y_probs, average=None, multi_class='ovr')\n",
    "    auc_macro = roc_auc_score(y_true_bin, y_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(\"\\nüéØ ROC AUC per class:\")\n",
    "    for i, cls in enumerate(class_names):\n",
    "        print(f\"Class {cls}: AUC = {auc_per_class[i]:.4f}\")\n",
    "\n",
    "    print(f\"\\nüåê Macro ROC AUC: {auc_macro:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ROC AUC could not be computed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
