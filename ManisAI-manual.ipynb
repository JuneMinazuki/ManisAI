{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a604edc0",
   "metadata": {},
   "source": [
    "# **ManisAI**\n",
    "This document provides guidance on how to use the model to run thru a testing directory to check the accuracy of the model.\n",
    "\n",
    "### **Required Files for Testing**\n",
    "1. Machine Learning Model\n",
    "2. Testing Directory with Images in 8 Different Kuih Folder (Kuih Talam, Kuih Seri Muka, Kuih Ubi Kayu, Kuih Kaswi Pandan, Kuih Ketayap, Onde-onde, Kuih Lapis, Kek Lapis)\n",
    "3. Class Labels File (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a53180",
   "metadata": {},
   "source": [
    "### 1. Setting Up the Environment\n",
    "\n",
    "Install the necessary libraries to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b257a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y numpy pandas tensorflow\n",
    "%pip install --no-cache-dir tensorflow==2.12.0 numpy==1.23.5 pandas==1.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851a87e",
   "metadata": {},
   "source": [
    "### 2. Importing Libraries\n",
    "\n",
    "Imports the necessary Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "import io\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f79883",
   "metadata": {},
   "source": [
    "### 3. Upload Model\n",
    "\n",
    "Change the 'model_filename' to where the ManisAI.pth is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b114ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'manisAI.pth'\n",
    "model = keras.models.load_model(model_filename)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6818a01",
   "metadata": {},
   "source": [
    "### Optional: Upload Label\n",
    "\n",
    "Change the 'label_filename' to where your label class file is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ae65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filename = 'labels.txt'\n",
    "labels = {}\n",
    "with open(labels_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            idx, label = line.strip().split(' ', 1)\n",
    "            labels[int(idx)] = label\n",
    "\n",
    "print(f\"Loaded {len(labels)} classes:\")\n",
    "for idx, label in labels.items():\n",
    "    print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348b95e",
   "metadata": {},
   "source": [
    "### 4. Access Testing Directory\n",
    "\n",
    "Change the 'test_dir' to where your testing directory is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02748080",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'Testing/'\n",
    "test_images = []\n",
    "for root, _, files in os.walk(test_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            test_images.append(os.path.join(root, file))\n",
    "\n",
    "test_images.sort()  # Sort to ensure consistent order\n",
    "print(f\"Found {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56141632",
   "metadata": {},
   "source": [
    "### 5. Running Predictions\n",
    "\n",
    "Process each image for model input (resize, normalize) and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "input_shape = model.input_shape[1:3]\n",
    "\n",
    "for img_path in tqdm(test_images):\n",
    "    try:\n",
    "        # Preprocess the image\n",
    "        img = image.load_img(img_path, target_size=input_shape)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = img_array / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Predict class probabilities\n",
    "        pred_probs = model.predict(img_array, verbose=0)[0]  # shape: (n_classes,)\n",
    "        predicted_class_idx = int(np.argmax(pred_probs))\n",
    "\n",
    "        # Get the label for the predicted class\n",
    "        predicted_label = labels.get(predicted_class_idx, f\"Unknown ({predicted_class_idx})\")\n",
    "\n",
    "        # Store prediction result\n",
    "        predictions.append({\n",
    "            'image': os.path.basename(img_path),\n",
    "            'predicted_class_index': predicted_class_idx,\n",
    "            'predicted_label': predicted_label,\n",
    "            'class_probabilities': pred_probs.tolist()  # convert to list for JSON-safe export\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {str(e)}\")\n",
    "        predictions.append({\n",
    "            'image': os.path.basename(img_path),\n",
    "            'predicted_class_index': -1,\n",
    "            'predicted_label': 'Error',\n",
    "            'class_probabilities': []\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283c38c",
   "metadata": {},
   "source": [
    "### 6. Creating Output\n",
    "\n",
    "Convert the model's prediction results into a structured format with each images':\n",
    "1. Predicted class index\n",
    "2. Predicted label\n",
    "3. Class probabilities\n",
    "4. True class index\n",
    "5. True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(predictions)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8c967",
   "metadata": {},
   "source": [
    "### 7. Metrics Computation\n",
    "\n",
    "Calculate the metrics of the model including:\n",
    "1. Model accuracy\n",
    "2. Precision, recall, and F1 for each class\n",
    "3. Macro precision\n",
    "4. Macro recall\n",
    "5. Macro F1\n",
    "6. ROC AUC per class\n",
    "7. Macro ROC AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score,\n",
    "    roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# True and predicted labels\n",
    "true_labels = [1, 2, 5]  # Adjust this list to match your full test set\n",
    "results_df['true_class_index'] = true_labels\n",
    "y_true = results_df['true_class_index'].astype(int).values\n",
    "y_pred = results_df['predicted_class_index'].astype(int).values\n",
    "y_probs = np.array(results_df['class_probabilities'].tolist())\n",
    "\n",
    "## Quick fix for ROC Curve as I only have 3 classes here (DO NOT NEED THIS IF YOU HAVE 8 CLASSES IN YOUR TEST SET)\n",
    "FULL_NUM_CLASSES = 8  # total number of possible classes\n",
    "\n",
    "# Pad probability vectors to length 8\n",
    "def pad_probs(probs, target_len=FULL_NUM_CLASSES):\n",
    "    padded = np.zeros(target_len)\n",
    "    padded[:len(probs)] = probs  # assumes probs are in order (class 0, 1, 2, ...)\n",
    "    return padded\n",
    "\n",
    "# Apply padding\n",
    "y_probs_padded = np.array([pad_probs(p, FULL_NUM_CLASSES) for p in results_df['class_probabilities']])\n",
    "\n",
    "# Update your DataFrame or use directly in metrics\n",
    "y_probs = y_probs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "n_classes = FULL_NUM_CLASSES\n",
    "class_names = list(range(FULL_NUM_CLASSES))\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n‚úÖ Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1 per class & macro\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=class_names, average=None)\n",
    "macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nüìä Per-class metrics:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"Class {cls}: Precision={prec[i]:.4f}, Recall={rec[i]:.4f}, F1={f1[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nüì¶ Macro Precision: {macro_prec:.4f}, Macro Recall: {macro_rec:.4f}, Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "# ROC AUC (requires binarized labels)\n",
    "y_true_bin = label_binarize(y_true, classes=class_names)\n",
    "\n",
    "# ROC AUC per class and macro\n",
    "try:\n",
    "    auc_per_class = roc_auc_score(y_true_bin, y_probs, average=None, multi_class='ovr')\n",
    "    auc_macro = roc_auc_score(y_true_bin, y_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(\"\\nüéØ ROC AUC per class:\")\n",
    "    for i, cls in enumerate(class_names):\n",
    "        print(f\"Class {cls}: AUC = {auc_per_class[i]:.4f}\")\n",
    "\n",
    "    print(f\"\\nüåê Macro ROC AUC: {auc_macro:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ROC AUC could not be computed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
