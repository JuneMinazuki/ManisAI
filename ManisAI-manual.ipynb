{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a604edc0",
   "metadata": {},
   "source": [
    "# **ManisAI**\n",
    "This document provides guidance on how to use the model to run thru a testing directory to check the accuracy of the model.\n",
    "\n",
    "### **Required Files for Testing**\n",
    "1. Machine Learning Model\n",
    "2. Testing Directory with Images in 8 Different Kuih Folder (Kuih Talam, Kuih Seri Muka, Kuih Ubi Kayu, Kuih Kaswi Pandan, Kuih Ketayap, Onde-onde, Kuih Lapis, Kek Lapis)\n",
    "3. Class Labels File (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a53180",
   "metadata": {},
   "source": [
    "### 1. Setting Up the Environment\n",
    "\n",
    "Install the necessary libraries to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b257a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y numpy pandas tensorflow\n",
    "%pip install --no-cache-dir tensorflow==2.12.0 numpy==1.23.5 pandas==1.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851a87e",
   "metadata": {},
   "source": [
    "### 2. Importing Libraries\n",
    "\n",
    "Imports the necessary Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "import io\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.functional as func\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score,\n",
    "    roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f79883",
   "metadata": {},
   "source": [
    "### 3. Upload Model\n",
    "\n",
    "Change the 'model_filename' to where the ManisAI.pth is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b114ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'manisAI.pth'\n",
    "model = torch.load(model_filename, map_location=torch.device('cpu'), weights_only=False)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6818a01",
   "metadata": {},
   "source": [
    "### Optional: Upload Label\n",
    "\n",
    "Change the 'label_filename' to where your label class file is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ae65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filename = 'labels.txt'\n",
    "labels = {}\n",
    "with open(labels_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            idx, label = line.strip().split(' ', 1)\n",
    "            labels[int(idx)] = label\n",
    "\n",
    "print(f\"Loaded {len(labels)} classes:\")\n",
    "for idx, label in labels.items():\n",
    "    print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348b95e",
   "metadata": {},
   "source": [
    "### 4. Access Testing Directory\n",
    "\n",
    "Change the 'test_dir' to where your testing directory is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02748080",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'Testing/'\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Found {len(val_dataset)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56141632",
   "metadata": {},
   "source": [
    "### 5. Running Predictions\n",
    "\n",
    "Process each image for model input (resize, normalize) and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_path in tqdm(range(len(val_dataset))):\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Apply softmax to get probabilities (along dimension 1, which is the class dimension)\n",
    "            probabilities = func.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                sample_probabilities = probabilities[i].tolist()\n",
    "                predictions.append(\n",
    "                    {\n",
    "                        'predicted_class_index': predicted[i],\n",
    "                        'predicted_label': class_names[predicted[i]],\n",
    "                        'class_probabilities': sample_probabilities,\n",
    "                        'true_class_index': labels[i],\n",
    "                        'true_label': class_names[labels[i]]\n",
    "                    }\n",
    "                )\n",
    "                print(f\"{n}/{(len(val_dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283c38c",
   "metadata": {},
   "source": [
    "### 6. Creating Output\n",
    "\n",
    "Convert the model's prediction results into a structured format with each images':\n",
    "1. Predicted class index\n",
    "2. Predicted label\n",
    "3. Class probabilities\n",
    "4. True class index\n",
    "5. True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(predictions)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8c967",
   "metadata": {},
   "source": [
    "### 7. Metrics Computation\n",
    "\n",
    "Calculate the metrics of the model including:\n",
    "1. Model accuracy\n",
    "2. Precision, recall, and F1 for each class\n",
    "3. Macro precision\n",
    "4. Macro recall\n",
    "5. Macro F1\n",
    "6. ROC AUC per class\n",
    "7. Macro ROC AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True and predicted labels\n",
    "y_true = results_df['true_class_index'].astype(int).values\n",
    "y_pred = results_df['predicted_class_index'].astype(int).values\n",
    "y_probs = np.array(results_df['class_probabilities'].tolist())\n",
    "\n",
    "## Quick fix for ROC Curve as I only have 3 classes here (DO NOT NEED THIS IF YOU HAVE 8 CLASSES IN YOUR TEST SET)\n",
    "FULL_NUM_CLASSES = 8  # total number of possible classes\n",
    "\n",
    "# Pad probability vectors to length 8\n",
    "def pad_probs(probs, target_len=FULL_NUM_CLASSES):\n",
    "    padded = np.zeros(target_len)\n",
    "    padded[:len(probs)] = probs  # assumes probs are in order (class 0, 1, 2, ...)\n",
    "    return padded\n",
    "\n",
    "# Apply padding\n",
    "y_probs_padded = np.array([pad_probs(p, FULL_NUM_CLASSES) for p in results_df['class_probabilities']])\n",
    "\n",
    "# Update your DataFrame or use directly in metrics\n",
    "y_probs = y_probs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "n_classes = FULL_NUM_CLASSES\n",
    "class_names = list(range(FULL_NUM_CLASSES))\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n‚úÖ Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1 per class & macro\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=class_names, average=None)\n",
    "macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nüìä Per-class metrics:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"Class {cls}: Precision={prec[i]:.4f}, Recall={rec[i]:.4f}, F1={f1[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nüì¶ Macro Precision: {macro_prec:.4f}, Macro Recall: {macro_rec:.4f}, Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "# ROC AUC (requires binarized labels)\n",
    "y_true_bin = label_binarize(y_true, classes=class_names)\n",
    "\n",
    "# ROC AUC per class and macro\n",
    "try:\n",
    "    auc_per_class = roc_auc_score(y_true_bin, y_probs, average=None, multi_class='ovr')\n",
    "    auc_macro = roc_auc_score(y_true_bin, y_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(\"\\nüéØ ROC AUC per class:\")\n",
    "    for i, cls in enumerate(class_names):\n",
    "        print(f\"Class {cls}: AUC = {auc_per_class[i]:.4f}\")\n",
    "\n",
    "    print(f\"\\nüåê Macro ROC AUC: {auc_macro:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ROC AUC could not be computed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
